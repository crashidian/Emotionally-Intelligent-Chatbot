{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzSW5Aj0qBxvIVTMnmpaZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crashidian/Emotionally-Intelligent-Chatbot/blob/main/emotionallyintelligentchatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torch_geometric transformers networkx scikit-learn spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-wQgPkUfkro",
        "outputId": "7347ff91-5bf9-41ef-e1ca-0e58c0e06b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torch_geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ctransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrBcThK4hA5L",
        "outputId": "d24fbcd7-648c-4f4d-d3f9-e371f1377429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers) (0.23.5)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2024.7.4)\n",
            "Installing collected packages: ctransformers\n",
            "Successfully installed ctransformers-0.2.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy"
      ],
      "metadata": {
        "id": "wAY63JSkhGk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmvLhuL8hjH5",
        "outputId": "ce7c1457-5f00-4f13-e26f-9a29e7f5f0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy for NLP tasks\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except ImportError:\n",
        "    print(\"Error: SpaCy model 'en_core_web_sm' not found.\")\n",
        "    print(\"Please run the following command in your terminal:\")\n",
        "    print(\"python -m spacy download en_core_web_sm\")\n",
        "    print(\"Then restart this script.\")\n",
        "    exit(1)"
      ],
      "metadata": {
        "id": "dwZ2OhVshnAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "X9SYa8lKiToQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GNN Model for context and storytelling\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, num_node_features)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        return self.conv3(x, edge_index)"
      ],
      "metadata": {
        "id": "_VFV6yGJktLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GNN model\n",
        "gnn_model = GNNModel(num_node_features=50, hidden_channels=64).to(device)\n",
        "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "VN0VaijBkxpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and maintain knowledge graph\n",
        "knowledge_graph = nx.Graph()"
      ],
      "metadata": {
        "id": "g2DfW6phkyX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorizer for text representation\n",
        "tfidf = TfidfVectorizer(max_features=50)"
      ],
      "metadata": {
        "id": "hutA8PX6k0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to update knowledge graph\n",
        "def update_knowledge_graph(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    for entity in entities:\n",
        "        if entity not in knowledge_graph:\n",
        "            knowledge_graph.add_node(entity, embedding=torch.rand(50, device=device))\n",
        "    for i, entity1 in enumerate(entities):\n",
        "        for entity2 in entities[i+1:]:\n",
        "            if not knowledge_graph.has_edge(entity1, entity2):\n",
        "                knowledge_graph.add_edge(entity1, entity2)"
      ],
      "metadata": {
        "id": "55S13yDfk2OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get graph embeddings\n",
        "def get_graph_embeddings():\n",
        "    if not knowledge_graph.nodes:\n",
        "        return torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    node_features = torch.stack([knowledge_graph.nodes[node]['embedding'] for node in knowledge_graph.nodes])\n",
        "    edges = list(knowledge_graph.edges())\n",
        "    if not edges:\n",
        "        return node_features, torch.tensor([], device=device)\n",
        "    edge_index = torch.tensor([[list(knowledge_graph.nodes()).index(edge[0]),\n",
        "                                list(knowledge_graph.nodes()).index(edge[1])]\n",
        "                               for edge in edges], device=device).t().contiguous()\n",
        "    return node_features, edge_index"
      ],
      "metadata": {
        "id": "bHCpMLI_k4X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion recognition with psychological context\n",
        "def recognize_emotion(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Basic emotion detection\n",
        "    emotion_keywords = {\n",
        "        'joy': ['happy', 'joy', 'excited', 'glad'],\n",
        "        'sadness': ['sad', 'upset', 'depressed', 'unhappy'],\n",
        "        'anger': ['angry', 'mad', 'furious', 'annoyed'],\n",
        "        'fear': ['afraid', 'scared', 'terrified', 'anxious'],\n",
        "        'surprise': ['surprised', 'shocked', 'amazed'],\n",
        "    }\n",
        "\n",
        "    detected_emotion = 'neutral'\n",
        "    for emotion, keywords in emotion_keywords.items():\n",
        "        if any(word in text.lower() for word in keywords):\n",
        "            detected_emotion = emotion\n",
        "            break\n",
        "\n",
        "    # Psychological context\n",
        "    sentiment = doc.sentiment\n",
        "    important_entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'EVENT']]\n",
        "\n",
        "    return {\n",
        "        'basic_emotion': detected_emotion,\n",
        "        'sentiment': sentiment,\n",
        "        'important_entities': important_entities\n",
        "    }"
      ],
      "metadata": {
        "id": "EBZDmxXck6vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate empathetic response using GPT-2\n",
        "def generate_empathetic_response(user_input, emotion_info, conversation_history):\n",
        "    try:\n",
        "        # Update knowledge graph\n",
        "        update_knowledge_graph(user_input)\n",
        "\n",
        "        # Get graph embeddings\n",
        "        node_features, edge_index = get_graph_embeddings()\n",
        "\n",
        "        print(\"Node features shape:\", node_features.shape)\n",
        "        print(\"Edge index shape:\", edge_index.shape)\n",
        "\n",
        "        # Use GNN to process graph if we have valid data\n",
        "        if node_features.nelement() > 0 and edge_index.nelement() > 0:\n",
        "            with torch.no_grad():\n",
        "                gnn_output = gnn_model(node_features, edge_index)\n",
        "        else:\n",
        "            print(\"Warning: Not enough data for GNN processing\")\n",
        "            gnn_output = torch.tensor([], device=device)\n",
        "\n",
        "        # Create context-aware prompt\n",
        "        context = \"Based on our conversation, I understand that:\\n\"\n",
        "        context += f\"- You're feeling {emotion_info.get('basic_emotion', 'neutral')}\\n\"\n",
        "        context += f\"- The sentiment of your message is {emotion_info.get('sentiment', 'neutral')}\\n\"\n",
        "        important_entities = emotion_info.get('important_entities', [])\n",
        "        if important_entities:\n",
        "            context += f\"- Important aspects mentioned: {', '.join(important_entities)}\\n\"\n",
        "        context += f\"Considering this context, respond empathetically to: {user_input}\"\n",
        "\n",
        "        full_prompt = f\"{conversation_history}\\nHuman: {context}\\nAI:\"\n",
        "\n",
        "        # Generate response using GPT-2\n",
        "        input_ids = tokenizer.encode(full_prompt, return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
        "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "        output = gpt2_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=min(len(input_ids[0]) + 100, 1024),\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract AI's response\n",
        "        ai_response = response.split(\"AI:\")[-1].strip()\n",
        "\n",
        "        return ai_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return \"I apologize, but I encountered an error while processing your input. Could you please try rephrasing your message?\"\n"
      ],
      "metadata": {
        "id": "tuT9D8Cbk942"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GNN model (simplified)\n",
        "def train_gnn():\n",
        "    gnn_model.train()\n",
        "    node_features, edge_index = get_graph_embeddings()\n",
        "    if node_features.nelement() > 0 and edge_index.nelement() > 0:\n",
        "        gnn_optimizer.zero_grad()\n",
        "        out = gnn_model(node_features, edge_index)\n",
        "        loss = F.mse_loss(out, node_features)\n",
        "        loss.backward()\n",
        "        gnn_optimizer.step()"
      ],
      "metadata": {
        "id": "IAyr8PDJlAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat function\n",
        "def interactive_chat():\n",
        "    print(\"Welcome to the GPT-2 GNN Empathetic Chatbot!\")\n",
        "    print(\"Type 'quit', 'exit', or 'bye' to end the conversation.\")\n",
        "\n",
        "    conversation_history = \"\"\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Chatbot: I understand you want to end our conversation. I hope our chat was helpful. Take care, and remember that it's okay to reach out if you need support. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        emotion_info = recognize_emotion(user_input)\n",
        "        response = generate_empathetic_response(user_input, emotion_info, conversation_history)\n",
        "        print(f\"Chatbot: {response}\")\n",
        "\n",
        "        # Update conversation history\n",
        "        conversation_history += f\"\\nHuman: {user_input}\\nAI: {response}\"\n",
        "\n",
        "        # Keep conversation history to a reasonable size\n",
        "        conversation_history = \"\\n\".join(conversation_history.split(\"\\n\")[-20:])\n",
        "\n",
        "        # Train GNN on updated knowledge graph\n",
        "        train_gnn()"
      ],
      "metadata": {
        "id": "XiJWJ9IclCc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFwvVzHJlEO0",
        "outputId": "de38dc2b-ecb1-444b-8051-e9131abc2187"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the GPT-2 GNN Empathetic Chatbot!\n",
            "Type 'quit', 'exit', or 'bye' to end the conversation.\n",
            "You: Hello. I am feeling very happy today because I woke up early and completed all of my chores.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features shape: torch.Size([1, 50])\n",
            "Edge index shape: torch.Size([0])\n",
            "Warning: Not enough data for GNN processing\n",
            "Chatbot: This sentiment is not 0.\n",
            "I'd like to know what is the sentiment that makes you feel happy, if any?\n",
            "Thank you.\n",
            "You: The reason why I am feeling happy is that I finished all of my housework before the start of my work day, including the sweeping, vacuuming, litter box, and other stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features shape: torch.Size([1, 50])\n",
            "Edge index shape: torch.Size([0])\n",
            "Warning: Not enough data for GNN processing\n",
            "Chatbot: Now, it's time to give you a task. What are you feeling right now? The result of the task you want to complete? Yes or no. You may use the \"yes\" or \"no\" response. A \"YES\" means that you're satisfied with the result. If you choose \"NO,\" you'll have to work harder to finish your task, but you can choose to start working on it again later. When you decide to take a break, give yourself a chance to\n",
            "You: I am feeling happy. The result of the task makes me happy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features shape: torch.Size([1, 50])\n",
            "Edge index shape: torch.Size([0])\n",
            "Warning: Not enough data for GNN processing\n",
            "Chatbot: Well, you chose \"I am happy.\"\n",
            "So, now we know that happiness is a state of mind, which is something we can influence by doing things. We can control our emotions, by feeling them, or by simply letting them go. In other words, we don't have any control over our feelings. But we do have control of our actions. Do you see, that when you get angry, the emotion is going to make you angry. So we must avoid anger. However,\n",
            "You: quit\n",
            "Chatbot: I understand you want to end our conversation. I hope our chat was helpful. Take care, and remember that it's okay to reach out if you need support. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}